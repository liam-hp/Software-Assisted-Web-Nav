{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Fully Custom.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOJ3O+y9F0HQwOv3GgCZ17t"},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"code","metadata":{"id":"bFZ87w7VIwOh","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621300406733,"user_tz":240,"elapsed":17219,"user":{"displayName":"Liam Patty","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjODq-4fGyQao1EtNNjTiwSm41wHZ5LkbC3HJVODw=s64","userId":"11631993201225092647"}},"outputId":"2cfc7963-8743-4c01-bf8d-4b68291003b7"},"source":["# connecting Colab to Google Drive\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=True)\n","\n","#import pathlib\n","#path = pathlib.Path(\"/content/gdrive/My Drive/Colab Notebooks/EXAMPLE/\")\n","#type(path)\n","path = \"/content/gdrive/My Drive/Colab Notebooks/Datasets\"\n","\n","import os\n","os.chdir(path)\n","os.getcwd()\n","os.listdir(path)"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["['.DS_Store', 'test', 'train', 'model_1', 'More Research.gdoc']"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"markdown","metadata":{"id":"_XqPPzySIQ_n"},"source":["https://mc.ai/using-google-colab-for-convolutional-neural-network-with-images/"]},{"cell_type":"markdown","metadata":{"id":"N2gPGTLoIUEU"},"source":["https://colab.research.google.com/drive/1a7V9Hc7ks0xxDbfCZl2_96DUGlCLuh00#scrollTo=T2GoIQXPdSkG"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wUzLx8nnLbCQ","executionInfo":{"status":"ok","timestamp":1621300426372,"user_tz":240,"elapsed":17726,"user":{"displayName":"Liam Patty","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjODq-4fGyQao1EtNNjTiwSm41wHZ5LkbC3HJVODw=s64","userId":"11631993201225092647"}},"outputId":"52de7ca7-96c1-44b8-f11a-bc4f34434a7d"},"source":["!pip install -q keras\n","import keras\n","from keras.layers import Input, Lambda, Dense, Flatten\n","from keras.models import Model\n","from keras.applications.vgg19 import VGG19\n","from keras.applications.vgg16 import preprocess_input\n","from keras.preprocessing import image\n","from keras.preprocessing.image import ImageDataGenerator\n","import numpy as np\n","from glob import glob\n","import matplotlib.pyplot as plt\n","import pathlib\n","\n","train_path = (path+\"/train/\")\n","test_path = (path+\"/test/\")\n","\n","image_count = len(list(pathlib.Path(train_path).glob('*/*.jpg')))\n","print(\"train has \"+ str(image_count) + \" images.\")\n","\n","image_count = len(list(pathlib.Path(test_path).glob('*/*.jpg')))\n","print(\"test has \"+ str(image_count) + \" images.\")\n","\n"],"execution_count":3,"outputs":[{"output_type":"stream","text":["train has 3019 images.\n","test has 667 images.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"yJSor2tfMgEd"},"source":["https://www.youtube.com/watch?v=Gvwuyx_F-28"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"An3XNdxYVG_6","executionInfo":{"status":"ok","timestamp":1621300430478,"user_tz":240,"elapsed":609,"user":{"displayName":"Liam Patty","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjODq-4fGyQao1EtNNjTiwSm41wHZ5LkbC3HJVODw=s64","userId":"11631993201225092647"}},"outputId":"e5f8d604-ca0a-435e-f68a-3ea9c46932db"},"source":["# use ImageDataGenerator to preprocess the data\n","from keras.preprocessing.image import ImageDataGenerator\n","\n","# augment the data that we have\n","\n","size = 30\n","\n","train_datagen = ImageDataGenerator(rescale = 1./255,\n","                                   shear_range = 0.2,\n","                                   zoom_range = 0.2)\n","test_datagen = ImageDataGenerator(rescale = 1./255,\n","                                  shear_range = 0.2,\n","                                  zoom_range = 0.2)\n","\n","# prepare training data \n","training_data = train_datagen.flow_from_directory(train_path,\n","                                                 #color_mode = \"grayscale\",\n","                                                 shuffle = True,\n","                                                 seed = 42,\n","                                                 target_size = (size, size),\n","                                                 batch_size = 10,\n","                                                 class_mode = 'binary',)\n","\n","# prepare test data\n","test_data = test_datagen.flow_from_directory(test_path,\n","                                            #color_mode = \"grayscale\",\n","                                            shuffle = True,\n","                                            seed = 42,\n","                                            target_size = (size, size),\n","                                            batch_size = 6,\n","                                            class_mode = 'binary')\n","\n","\n","# to improve the model accuracy you can increase the number of steps_per_epoch"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Found 3019 images belonging to 3 classes.\n","Found 667 images belonging to 3 classes.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"lW8V5pKOT5GL"},"source":["print(type(training_data))\n","#__getitem__(idx)\n","from __future__ import print_function\n","import numpy as np\n","\n","def valid_imshow_data(data):\n","    data = np.asarray(data)\n","    if data.ndim == 2:\n","        return True\n","    elif data.ndim == 3:\n","        if 3 <= data.shape[2] <= 4:\n","            return True\n","        else:\n","            print('The \"data\" has 3 dimensions but the last dimension '\n","                  'must have a length of 3 (RGB) or 4 (RGBA), not \"{}\".'\n","                  ''.format(data.shape[2]))\n","            return False\n","    else:\n","        print('To visualize an image the data must be 2 dimensional or '\n","              '3 dimensional, not \"{}\".'\n","              ''.format(data.ndim))\n","        return False\n","\n","x,y = training_data.next()\n","for i in range(0,5):\n","    valid_imshow_data(image)\n","    image = x[i]\n","    plt.imshow(image)\n","    plt.show()\n","\n","#https://stackoverflow.com/questions/41695844/keras-showing-images-from-data-generator\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vK0IK8Q7F-3W","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621300441278,"user_tz":240,"elapsed":996,"user":{"displayName":"Liam Patty","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjODq-4fGyQao1EtNNjTiwSm41wHZ5LkbC3HJVODw=s64","userId":"11631993201225092647"}},"outputId":"004ee563-cd9b-4528-eb68-e6302e8701f5"},"source":["from keras.models import Sequential\n","from keras.layers import Dense, Dropout, Activation, Flatten # !!!\n","from keras.layers import Conv2D, MaxPooling2D # !!!\n","from keras.optimizers import SGD\n","\n","#idea: train 3(?) seperate neural networks and use them to check one another (majority wins)\n","\n","no_classes=3\n","\n","def model_1():\n","  model = Sequential()\n","  model.add(Conv2D(32, (3, 3), input_shape=(size, size, 3), activation=\"relu\")) #change input shape if grayscale\n","  model.add(MaxPooling2D(pool_size = (2, 2)))\n","  model.add(Flatten())\n","  model.add(Dense(activation=\"relu\", units=16))\n","  model.add(Dropout(.2))\n","  model.add(Dense(activation=\"sigmoid\", units=16))\n","  model.add(Dense(units=no_classes, activation='softmax'))\n","  model.compile(optimizer = keras.optimizers.Adam(lr=0.001), loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n","  return model\n","\n","def model_2():\n","  model = Sequential()\n","  model.add(Conv2D(32, (3, 3), input_shape=(size, size, 3), activation=\"relu\")) #change input shape if grayscale\n","  model.add(MaxPooling2D(pool_size = (2, 2)))\n","  model.add(Flatten())\n","  model.add(Dropout(.2))\n","  model.add(Dense(units=no_classes, activation='softmax'))\n","  model.compile(optimizer = keras.optimizers.Adam(lr=0.001), loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n","  return model\n","\n","def model_3():\n","\n","  # https://towardsdatascience.com/covolutional-neural-network-cb0883dd6529\n","\n","  model = Sequential()\n","    # The first two layers with 32 filters of window size 3x3\n","  model.add(Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=(size,size,3))) # conv2d layer with relu activation\n","  model.add(Conv2D(32, (3, 3), activation='relu'))# application of weights (ie filter/kernel) and biases as well as a final squishification function\n","  model.add(MaxPooling2D(pool_size=(2, 2)))# reduces dimensions\n","  model.add(Dropout(0.25))# prevents overfitting by setting 25% of neurons to 0\n","\n","  model.add(Conv2D(64, (3, 3), padding='same', activation='relu')) # conv2d layer with relu activation\n","  model.add(Conv2D(64, (3, 3), activation='relu'))# application of weights (ie filter/kernel) and biases as well as a final squishification function\n","  model.add(MaxPooling2D(pool_size=(2, 2)))# reduces dimensions\n","  model.add(Dropout(0.25))# prevents overfitting by setting 25% of neurons to 0\n","\n","  model.add(Conv2D(64, (3, 3), padding='same', activation='relu')) # conv2d layer with relu activation\n","  model.add(Conv2D(64, (3, 3), activation='relu'))# application of weights (ie filter/kernel) and biases as well as a final squishification function\n","  model.add(MaxPooling2D(pool_size=(2, 2)))# reduces dimensions | take the max value over a 2x2 pooling window\n","  model.add(Dropout(0.25))# prevents overfitting by setting 25% of neurons to 0\n","\n","  model.add(Flatten())# converting the data into a 1-dimensional array\n","  model.add(Dense(512, activation='relu'))# deeply connected layer\n","  model.add(Dropout(0.5))# prevents overfitting by setting 50% of neurons to 0\n","  model.add(Dense(no_classes, activation='softmax')) # deeply connected layer\n","  model.compile(optimizer = keras.optimizers.Adam(lr=0.001), loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n","          # compiling with defined optimizer and loss function (determine learning)\n","  \n","  #model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n","  return model\n","\n","model = model_3()\n","model.summary()"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Model: \"sequential\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","conv2d (Conv2D)              (None, 30, 30, 32)        896       \n","_________________________________________________________________\n","conv2d_1 (Conv2D)            (None, 28, 28, 32)        9248      \n","_________________________________________________________________\n","max_pooling2d (MaxPooling2D) (None, 14, 14, 32)        0         \n","_________________________________________________________________\n","dropout (Dropout)            (None, 14, 14, 32)        0         \n","_________________________________________________________________\n","conv2d_2 (Conv2D)            (None, 14, 14, 64)        18496     \n","_________________________________________________________________\n","conv2d_3 (Conv2D)            (None, 12, 12, 64)        36928     \n","_________________________________________________________________\n","max_pooling2d_1 (MaxPooling2 (None, 6, 6, 64)          0         \n","_________________________________________________________________\n","dropout_1 (Dropout)          (None, 6, 6, 64)          0         \n","_________________________________________________________________\n","conv2d_4 (Conv2D)            (None, 6, 6, 64)          36928     \n","_________________________________________________________________\n","conv2d_5 (Conv2D)            (None, 4, 4, 64)          36928     \n","_________________________________________________________________\n","max_pooling2d_2 (MaxPooling2 (None, 2, 2, 64)          0         \n","_________________________________________________________________\n","dropout_2 (Dropout)          (None, 2, 2, 64)          0         \n","_________________________________________________________________\n","flatten (Flatten)            (None, 256)               0         \n","_________________________________________________________________\n","dense (Dense)                (None, 512)               131584    \n","_________________________________________________________________\n","dropout_3 (Dropout)          (None, 512)               0         \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 3)                 1539      \n","=================================================================\n","Total params: 272,547\n","Trainable params: 272,547\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"2Q-89PBLzL1O","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621301345727,"user_tz":240,"elapsed":897803,"user":{"displayName":"Liam Patty","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjODq-4fGyQao1EtNNjTiwSm41wHZ5LkbC3HJVODw=s64","userId":"11631993201225092647"}},"outputId":"923dacf3-41a1-46cb-e211-68b77db6fb33"},"source":["# optimizers: https://www.dlology.com/blog/quick-notes-on-how-to-choose-optimizer-in-keras/\n","# 3019 (10), 667 (6)\n","\n","train_count = len(list(pathlib.Path(train_path).glob('*/*.jpg'))) # 3019\n","test_count = len(list(pathlib.Path(test_path).glob('*/*.jpg'))) # 667\n","# calculate ideal steps, epochs, and validation_steps\n","\n","#first train\n","history = model.fit(training_data,\n","                         steps_per_epoch = 60, # 37\n","                         epochs = 50, #10\n","                         verbose = 1,\n","                         validation_data = test_data, \n","                         validation_steps = 12) # 21"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Epoch 1/50\n","60/60 [==============================] - 139s 2s/step - loss: 1.1045 - accuracy: 0.3860 - val_loss: 1.0922 - val_accuracy: 0.5139\n","Epoch 2/50\n","60/60 [==============================] - 102s 2s/step - loss: 1.0098 - accuracy: 0.4316 - val_loss: 0.7511 - val_accuracy: 0.5556\n","Epoch 3/50\n","60/60 [==============================] - 85s 1s/step - loss: 0.6700 - accuracy: 0.7156 - val_loss: 0.8451 - val_accuracy: 0.6389\n","Epoch 4/50\n","60/60 [==============================] - 72s 1s/step - loss: 0.6165 - accuracy: 0.7637 - val_loss: 0.4899 - val_accuracy: 0.8472\n","Epoch 5/50\n","60/60 [==============================] - 52s 876ms/step - loss: 0.4436 - accuracy: 0.8411 - val_loss: 0.3632 - val_accuracy: 0.8750\n","Epoch 6/50\n","60/60 [==============================] - 40s 681ms/step - loss: 0.3131 - accuracy: 0.8992 - val_loss: 0.2867 - val_accuracy: 0.8889\n","Epoch 7/50\n","60/60 [==============================] - 43s 714ms/step - loss: 0.3944 - accuracy: 0.8898 - val_loss: 0.3586 - val_accuracy: 0.8472\n","Epoch 8/50\n","60/60 [==============================] - 31s 517ms/step - loss: 0.2677 - accuracy: 0.8944 - val_loss: 0.3867 - val_accuracy: 0.8889\n","Epoch 9/50\n","60/60 [==============================] - 27s 462ms/step - loss: 0.2644 - accuracy: 0.9218 - val_loss: 0.4802 - val_accuracy: 0.8611\n","Epoch 10/50\n","60/60 [==============================] - 21s 347ms/step - loss: 0.2763 - accuracy: 0.9210 - val_loss: 0.1905 - val_accuracy: 0.9444\n","Epoch 11/50\n","60/60 [==============================] - 20s 333ms/step - loss: 0.2001 - accuracy: 0.9454 - val_loss: 0.4382 - val_accuracy: 0.8611\n","Epoch 12/50\n","60/60 [==============================] - 14s 239ms/step - loss: 0.2556 - accuracy: 0.8929 - val_loss: 0.2918 - val_accuracy: 0.8889\n","Epoch 13/50\n","60/60 [==============================] - 16s 264ms/step - loss: 0.1980 - accuracy: 0.9181 - val_loss: 0.2827 - val_accuracy: 0.9167\n","Epoch 14/50\n","60/60 [==============================] - 11s 193ms/step - loss: 0.1617 - accuracy: 0.9361 - val_loss: 0.1659 - val_accuracy: 0.9167\n","Epoch 15/50\n","60/60 [==============================] - 14s 231ms/step - loss: 0.1884 - accuracy: 0.9416 - val_loss: 0.3606 - val_accuracy: 0.8611\n","Epoch 16/50\n","60/60 [==============================] - 10s 170ms/step - loss: 0.1802 - accuracy: 0.9395 - val_loss: 0.3167 - val_accuracy: 0.8889\n","Epoch 17/50\n","60/60 [==============================] - 9s 154ms/step - loss: 0.1286 - accuracy: 0.9557 - val_loss: 0.3029 - val_accuracy: 0.8889\n","Epoch 18/50\n","60/60 [==============================] - 8s 129ms/step - loss: 0.1882 - accuracy: 0.9348 - val_loss: 0.2068 - val_accuracy: 0.9444\n","Epoch 19/50\n","60/60 [==============================] - 8s 136ms/step - loss: 0.2166 - accuracy: 0.9320 - val_loss: 0.4100 - val_accuracy: 0.8750\n","Epoch 20/50\n","60/60 [==============================] - 8s 141ms/step - loss: 0.1170 - accuracy: 0.9585 - val_loss: 0.0803 - val_accuracy: 0.9722\n","Epoch 21/50\n","60/60 [==============================] - 7s 124ms/step - loss: 0.1275 - accuracy: 0.9648 - val_loss: 0.5687 - val_accuracy: 0.8750\n","Epoch 22/50\n","60/60 [==============================] - 7s 114ms/step - loss: 0.1268 - accuracy: 0.9565 - val_loss: 0.1755 - val_accuracy: 0.9583\n","Epoch 23/50\n","60/60 [==============================] - 7s 116ms/step - loss: 0.1342 - accuracy: 0.9554 - val_loss: 0.2161 - val_accuracy: 0.9583\n","Epoch 24/50\n","60/60 [==============================] - 7s 111ms/step - loss: 0.1000 - accuracy: 0.9574 - val_loss: 0.4247 - val_accuracy: 0.8750\n","Epoch 25/50\n","60/60 [==============================] - 6s 98ms/step - loss: 0.1068 - accuracy: 0.9627 - val_loss: 0.2790 - val_accuracy: 0.9306\n","Epoch 26/50\n","60/60 [==============================] - 6s 99ms/step - loss: 0.0901 - accuracy: 0.9701 - val_loss: 0.3405 - val_accuracy: 0.9306\n","Epoch 27/50\n","60/60 [==============================] - 5s 88ms/step - loss: 0.1276 - accuracy: 0.9600 - val_loss: 0.1382 - val_accuracy: 0.9583\n","Epoch 28/50\n","60/60 [==============================] - 5s 80ms/step - loss: 0.1224 - accuracy: 0.9640 - val_loss: 0.3582 - val_accuracy: 0.9028\n","Epoch 29/50\n","60/60 [==============================] - 5s 81ms/step - loss: 0.1141 - accuracy: 0.9634 - val_loss: 0.2411 - val_accuracy: 0.9028\n","Epoch 30/50\n","60/60 [==============================] - 6s 92ms/step - loss: 0.1291 - accuracy: 0.9480 - val_loss: 0.3369 - val_accuracy: 0.9444\n","Epoch 31/50\n","60/60 [==============================] - 5s 87ms/step - loss: 0.0869 - accuracy: 0.9586 - val_loss: 0.1948 - val_accuracy: 0.9306\n","Epoch 32/50\n","60/60 [==============================] - 5s 84ms/step - loss: 0.0944 - accuracy: 0.9646 - val_loss: 0.1873 - val_accuracy: 0.9722\n","Epoch 33/50\n","60/60 [==============================] - 5s 83ms/step - loss: 0.0474 - accuracy: 0.9798 - val_loss: 0.0850 - val_accuracy: 0.9861\n","Epoch 34/50\n","60/60 [==============================] - 5s 86ms/step - loss: 0.0733 - accuracy: 0.9778 - val_loss: 0.2369 - val_accuracy: 0.9583\n","Epoch 35/50\n","60/60 [==============================] - 5s 89ms/step - loss: 0.0605 - accuracy: 0.9772 - val_loss: 0.2682 - val_accuracy: 0.9583\n","Epoch 36/50\n","60/60 [==============================] - 5s 87ms/step - loss: 0.0960 - accuracy: 0.9699 - val_loss: 0.2022 - val_accuracy: 0.9167\n","Epoch 37/50\n","60/60 [==============================] - 6s 93ms/step - loss: 0.1020 - accuracy: 0.9634 - val_loss: 0.2027 - val_accuracy: 0.9306\n","Epoch 38/50\n","60/60 [==============================] - 5s 87ms/step - loss: 0.0532 - accuracy: 0.9807 - val_loss: 0.1058 - val_accuracy: 0.9722\n","Epoch 39/50\n","60/60 [==============================] - 5s 87ms/step - loss: 0.0687 - accuracy: 0.9786 - val_loss: 0.0625 - val_accuracy: 0.9722\n","Epoch 40/50\n","60/60 [==============================] - 5s 83ms/step - loss: 0.0571 - accuracy: 0.9786 - val_loss: 0.1134 - val_accuracy: 0.9722\n","Epoch 41/50\n","60/60 [==============================] - 5s 83ms/step - loss: 0.1011 - accuracy: 0.9604 - val_loss: 0.1373 - val_accuracy: 0.9722\n","Epoch 42/50\n","60/60 [==============================] - 5s 81ms/step - loss: 0.0989 - accuracy: 0.9652 - val_loss: 0.2788 - val_accuracy: 0.9306\n","Epoch 43/50\n","60/60 [==============================] - 5s 80ms/step - loss: 0.0674 - accuracy: 0.9845 - val_loss: 0.0624 - val_accuracy: 0.9722\n","Epoch 44/50\n","60/60 [==============================] - 5s 79ms/step - loss: 0.0796 - accuracy: 0.9693 - val_loss: 0.3418 - val_accuracy: 0.9028\n","Epoch 45/50\n","60/60 [==============================] - 5s 83ms/step - loss: 0.1367 - accuracy: 0.9668 - val_loss: 0.1725 - val_accuracy: 0.9583\n","Epoch 46/50\n","60/60 [==============================] - 5s 82ms/step - loss: 0.0771 - accuracy: 0.9653 - val_loss: 0.1715 - val_accuracy: 0.9444\n","Epoch 47/50\n","60/60 [==============================] - 5s 86ms/step - loss: 0.1027 - accuracy: 0.9670 - val_loss: 0.3448 - val_accuracy: 0.8750\n","Epoch 48/50\n","60/60 [==============================] - 5s 83ms/step - loss: 0.0771 - accuracy: 0.9791 - val_loss: 0.2798 - val_accuracy: 0.9583\n","Epoch 49/50\n","60/60 [==============================] - 5s 78ms/step - loss: 0.0817 - accuracy: 0.9700 - val_loss: 0.1256 - val_accuracy: 0.9444\n","Epoch 50/50\n","60/60 [==============================] - 5s 77ms/step - loss: 0.0771 - accuracy: 0.9721 - val_loss: 0.1815 - val_accuracy: 0.9583\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"IwI_meV96isR","colab":{"base_uri":"https://localhost:8080/","height":232},"executionInfo":{"status":"error","timestamp":1621300370723,"user_tz":240,"elapsed":1140,"user":{"displayName":"Liam Patty","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjODq-4fGyQao1EtNNjTiwSm41wHZ5LkbC3HJVODw=s64","userId":"11631993201225092647"}},"outputId":"4e7c63fb-e234-49d9-ea9c-4326e93d6d44"},"source":["loss = history.history['loss']\n","val_loss = history.history['val_loss']\n","epochs = range(1, len(loss) + 1)\n","plt.plot(epochs, loss, 'y', label='Training loss')\n","plt.plot(epochs, val_loss, 'r', label='Validation loss')\n","plt.title('Training and validation loss')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.legend()\n","plt.show()\n","\n","#test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)"],"execution_count":1,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-35077c725dc9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'y'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Training loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Validation loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"]}]},{"cell_type":"code","metadata":{"id":"FM-3wO9L8kNt","executionInfo":{"status":"aborted","timestamp":1621300370720,"user_tz":240,"elapsed":1120,"user":{"displayName":"Liam Patty","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjODq-4fGyQao1EtNNjTiwSm41wHZ5LkbC3HJVODw=s64","userId":"11631993201225092647"}}},"source":["acc = history.history['accuracy']\n","val_acc = history.history['val_accuracy']\n","epochs = range(1, len(loss) + 1)\n","plt.plot(epochs, acc, 'y', label='Training acc')\n","plt.plot(epochs, val_acc, 'r', label='Validation acc')\n","plt.title('Training and validation accuracy')\n","plt.xlabel('Epochs')\n","plt.ylabel('Accuracy')\n","plt.legend()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7mBJQPMPMswS"},"source":["# second train\n","history = model.fit(training_data,\n","                         steps_per_epoch = 301,\n","                         epochs = 10,\n","                         verbose = 1,\n","                         validation_data = test_data, \n","                         validation_steps = 66)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZMxBIPD8-WiI"},"source":["'''\n","\n","saved models:\n","\n","model_1 | current model3, trained on 50 epochs with the last two yeilding 100% accuracy, choosing between search, bookmark, and calendar | 3/9/21\n","\n","'''"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6nemwqZx6NSa"},"source":["loc = \"model_1\" #loc2 = \"/Users/pattyl21/Desktop\" location = \"/content/gdrive/My Drive/12th Grade 2020-21/Idea #2 (technology adapter):/Patty-William (Liam), STEM/model_1.0\"\n","\n","recon = keras.models.load_model(loc)\n","\n","recon.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"v2WboMLx_YJv"},"source":["\n","(image_batch_train, label_batch_train) = next(iter(training_data))\n","print(label_batch_train)\n","print(type (label_batch_train))\n","\n","import numpy as np\n","from keras.preprocessing import image\n","def predict_ten(ibt, mdl):\n","  prediction = mdl.predict(ibt[:10])\n","  results = [1,2,3,4,5,6,7,8,9,10]\n","  a=0; correct = 0\n","  for i in prediction:\n","    gpos = -1; inc = 0; saved_inc=0\n","    for s in i:\n","      if (s > gpos):\n","        gpos = s\n","        saved_inc=inc\n","      inc+=1\n","    results[a] = saved_inc\n","    a+=1\n","  return results\n","\n","\n","print(predict_ten(image_batch_train,recon))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gZDbU643dWye","executionInfo":{"status":"ok","timestamp":1615395440602,"user_tz":300,"elapsed":25063,"user":{"displayName":"Liam Patty","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjODq-4fGyQao1EtNNjTiwSm41wHZ5LkbC3HJVODw=s64","userId":"11631993201225092647"}},"outputId":"9312682f-0599-4212-b2fe-e38c32bb6cb4"},"source":["i=0; correct=0; total=0\n","classes = [\"bookmark\", \"calendar\", \"search\"]\n","while (i<10):\n","  j=0; (image_batch_train, label_batch_train) = next(iter(training_data))\n","  while j < len(image_batch_train):\n","    if (predict_ten(image_batch_train, recon)[j] == label_batch_train[j]):\n","      correct+=1\n","    else:\n","      plt.imshow(image_batch_train[j])\n","      plt.show()\n","      print(\"The CNN incorrectly recognized this image as: \" + classes[int(predict_ten(image_batch_train, recon)[j])] + \", while it was actually \"+ classes[int(label_batch_train[j])])\n","    total+=1; j+=1\n","  i+=1\n","print(\"Accuracy: \"+ str(correct) + \"/\" + str(total))\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Accuracy: 100/100\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jFAm5FnEBGwS"},"source":["n = 0\n","for i in image_batch_train[:10]:\n","  print(\"\\n\")\n","  print(label_batch_train[n])\n","  print(\".\")\n","  plt.imshow(i)\n","  plt.show()\n","  n+=1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6NAqLC7v6HBf","executionInfo":{"status":"ok","timestamp":1615325606309,"user_tz":300,"elapsed":3105,"user":{"displayName":"Liam Patty","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjODq-4fGyQao1EtNNjTiwSm41wHZ5LkbC3HJVODw=s64","userId":"11631993201225092647"}},"outputId":"00596210-b471-4f15-d3c9-19d25d7bb468"},"source":["# Saving the model\n","\n","model.save(\"model_\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:Assets written to: model_1/assets\n"],"name":"stdout"}]}]}